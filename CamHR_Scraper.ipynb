{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18dfc6ff",
   "metadata": {},
   "source": [
    "# CamHR.com Job Scraper\n",
    "\n",
    "This notebook contains a comprehensive web scraper for extracting job listings from CamHR.com (Cambodia Human Resources). The scraper systematically collects detailed job information including job titles, company details, requirements, qualifications, and employment terms.\n",
    "\n",
    "## Features\n",
    "- **Advanced Web Scraping**: Uses Selenium WebDriver with BeautifulSoup for comprehensive data extraction\n",
    "- **Robust Data Collection**: Extracts 18 different job attributes\n",
    "- **Error Handling**: Graceful handling of missing pages and elements\n",
    "- **CSV Export**: Structured data export with UTF-8 encoding\n",
    "- **Progress Tracking**: Real-time scraping progress updates\n",
    "- **Headless Operation**: Efficient background processing\n",
    "\n",
    "## Data Fields Extracted\n",
    "- **Basic Info**: Job Title, Company Name, Link URL\n",
    "- **Job Level**: Level, Years of Experience\n",
    "- **Employment Terms**: Hiring status, Salary, Employment Term\n",
    "- **Demographics**: Sex, Age requirements\n",
    "- **Classification**: Function, Industry\n",
    "- **Requirements**: Qualification, Language, Location\n",
    "- **Detailed Info**: Job Requirements\n",
    "- **Timeline**: Publish Date, Closing Date\n",
    "\n",
    "## Requirements\n",
    "- Python 3.x\n",
    "- Selenium WebDriver\n",
    "- BeautifulSoup4\n",
    "- Pandas\n",
    "- Chrome WebDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a34dc31",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for web scraping, data processing, and file handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4403a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Scraping session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46813d1",
   "metadata": {},
   "source": [
    "## 2. Configuration Class\n",
    "\n",
    "Define a configuration class containing all scraper settings for easy customization and maintenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac982bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamHRConfig:\n",
    "    \"\"\"Configuration class for CamHR job scraper\"\"\"\n",
    "    \n",
    "    # WebDriver settings\n",
    "    CHROME_DRIVER_PATH = r\"D:\\DSE_Folder\\Year_3\\Sem_2\\Web Scraping\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "    \n",
    "    # Scraping range\n",
    "    START_ID = 10611925\n",
    "    END_ID = 10613636\n",
    "    \n",
    "    # URL configuration\n",
    "    BASE_URL = \"https://www.camhr.com/a/job/{}\"\n",
    "    \n",
    "    # Output settings\n",
    "    CSV_FILENAME = \"New_Data_cam_4.csv\"\n",
    "    \n",
    "    # Timing settings\n",
    "    WAIT_TIMEOUT = 5  # seconds to wait for page elements\n",
    "    DELAY = 0.0000001  # delay between requests\n",
    "    \n",
    "    # CSV column definitions\n",
    "    COLUMNS = [\n",
    "        \"Job Title\", \"Company Name\", \"Level\", \"Year of Exp.\", \"Hiring\", \"Salary\", \"Sex\", \"Age\",\n",
    "        \"Term\", \"Function\", \"Industry\", \"Qualification\", \"Language\", \"Location\", \"Job Requirements\",\n",
    "        \"Publish Date\", \"Closing Date\", \"Link URL\"\n",
    "    ]\n",
    "\n",
    "# Initialize configuration\n",
    "config = CamHRConfig()\n",
    "\n",
    "print(\"üîß CamHR Scraper Configuration:\")\n",
    "print(f\"üìä Job ID range: {config.START_ID} to {config.END_ID}\")\n",
    "print(f\"üìÅ Output file: {config.CSV_FILENAME}\")\n",
    "print(f\"‚è±Ô∏è Wait timeout: {config.WAIT_TIMEOUT} seconds\")\n",
    "print(f\"üìù Data fields: {len(config.COLUMNS)} columns\")\n",
    "print(f\"üîó Base URL: {config.BASE_URL}\")\n",
    "print(f\"üìà Total jobs to scrape: {config.END_ID - config.START_ID + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3b0a3",
   "metadata": {},
   "source": [
    "## 3. WebDriver Setup and Initialization\n",
    "\n",
    "Configure and initialize the Chrome WebDriver with optimized settings for efficient scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_chrome_driver(config):\n",
    "    \"\"\"\n",
    "    Initialize Chrome WebDriver with optimized settings\n",
    "    \n",
    "    Args:\n",
    "        config: CamHRConfig instance\n",
    "    \n",
    "    Returns:\n",
    "        webdriver.Chrome: Configured Chrome WebDriver\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set up Chrome options for optimal performance\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")              # Run without GUI\n",
    "        chrome_options.add_argument(\"--disable-gpu\")           # Disable GPU acceleration\n",
    "        chrome_options.add_argument(\"--no-sandbox\")            # Bypass OS security model\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\") # Overcome limited resource problems\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")  # Set window size\n",
    "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\") # Avoid detection\n",
    "        \n",
    "        # Initialize WebDriver service\n",
    "        service = Service(config.CHROME_DRIVER_PATH)\n",
    "        \n",
    "        # Create WebDriver instance\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "        print(\"‚úÖ Chrome WebDriver initialized successfully!\")\n",
    "        print(f\"üåê Browser version: {driver.capabilities.get('browserVersion', 'Unknown')}\")\n",
    "        print(f\"üîß ChromeDriver version: {driver.capabilities.get('chrome', {}).get('chromedriverVersion', 'Unknown').split(' ')[0]}\")\n",
    "        \n",
    "        return driver\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing WebDriver: {e}\")\n",
    "        return None\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = setup_chrome_driver(config)\n",
    "\n",
    "if driver:\n",
    "    print(\"üöÄ WebDriver ready for scraping!\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to initialize WebDriver. Please check the Chrome driver path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf05fa",
   "metadata": {},
   "source": [
    "## 4. Data Extraction Functions\n",
    "\n",
    "Define specialized functions for extracting different types of job information from the CamHR pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6f7610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_title(soup):\n",
    "    \"\"\"\n",
    "    Extract job title from the page\n",
    "    \n",
    "    Args:\n",
    "        soup: BeautifulSoup object\n",
    "    \n",
    "    Returns:\n",
    "        str: Job title or 'Not found'\n",
    "    \"\"\"\n",
    "    job_title_span = soup.find(\"span\", class_=\"job-name-span\")\n",
    "    return job_title_span.text.strip() if job_title_span else \"Not found\"\n",
    "\n",
    "def extract_company_name(soup):\n",
    "    \"\"\"\n",
    "    Extract company name from the page\n",
    "    \n",
    "    Args:\n",
    "        soup: BeautifulSoup object\n",
    "    \n",
    "    Returns:\n",
    "        str: Company name or 'Not found'\n",
    "    \"\"\"\n",
    "    company_name_tag = soup.find(\"p\", class_=\"mb-1 company-headbox\")\n",
    "    if company_name_tag:\n",
    "        company_link = company_name_tag.find(\"a\")\n",
    "        return company_link.text.strip() if company_link else \"Not found\"\n",
    "    return \"Not found\"\n",
    "\n",
    "def extract_table_data(soup, columns):\n",
    "    \"\"\"\n",
    "    Extract job details from the information table\n",
    "    \n",
    "    Args:\n",
    "        soup: BeautifulSoup object\n",
    "        columns: List of column names to match\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary mapping column names to values\n",
    "    \"\"\"\n",
    "    table_data = {}\n",
    "    \n",
    "    table = soup.find(\"table\", class_=\"mailTable\")\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            headers = row.find_all(\"th\", class_=\"column\")\n",
    "            data_cells = row.find_all(\"td\")\n",
    "            \n",
    "            for header, data in zip(headers, data_cells):\n",
    "                key = header.text.strip()\n",
    "                value = data.text.strip()\n",
    "                \n",
    "                # Match table headers with CSV columns\n",
    "                for column in columns:\n",
    "                    if key.lower() in column.lower():\n",
    "                        table_data[column] = value\n",
    "                        break\n",
    "    \n",
    "    return table_data\n",
    "\n",
    "def extract_job_requirements(soup):\n",
    "    \"\"\"\n",
    "    Extract detailed job requirements\n",
    "    \n",
    "    Args:\n",
    "        soup: BeautifulSoup object\n",
    "    \n",
    "    Returns:\n",
    "        str: Job requirements or 'Not found'\n",
    "    \"\"\"\n",
    "    job_descript_divs = soup.find_all(\"div\", class_=\"job-descript\")\n",
    "    \n",
    "    for div in job_descript_divs:\n",
    "        title_span = div.find(\"span\", class_=\"descript-title\")\n",
    "        if title_span and \"Job Requirements\" in title_span.text:\n",
    "            requirements_div = div.find(\"div\", class_=\"fs-14 descript-list\")\n",
    "            if requirements_div:\n",
    "                return requirements_div.get_text(separator=\"\\n\").strip()\n",
    "    \n",
    "    return \"Not found\"\n",
    "\n",
    "def extract_dates(soup):\n",
    "    \"\"\"\n",
    "    Extract publish date and closing date\n",
    "    \n",
    "    Args:\n",
    "        soup: BeautifulSoup object\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (publish_date, closing_date)\n",
    "    \"\"\"\n",
    "    send_date_div = soup.find(\"div\", class_=\"send-date\")\n",
    "    \n",
    "    if send_date_div:\n",
    "        date_spans = send_date_div.find_all(\"span\")\n",
    "        if len(date_spans) >= 2:\n",
    "            publish_date = date_spans[0].text.split(\": \")[-1].strip()\n",
    "            closing_date = date_spans[1].text.split(\": \")[-1].strip()\n",
    "            return publish_date, closing_date\n",
    "    \n",
    "    return \"Not found\", \"Not found\"\n",
    "\n",
    "print(\"‚úÖ Data extraction functions defined successfully!\")\n",
    "print(\"üîß Available functions:\")\n",
    "print(\"   - extract_job_title(): Job title extraction\")\n",
    "print(\"   - extract_company_name(): Company name extraction\")\n",
    "print(\"   - extract_table_data(): Table-based data extraction\")\n",
    "print(\"   - extract_job_requirements(): Detailed requirements extraction\")\n",
    "print(\"   - extract_dates(): Publish and closing date extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2c079",
   "metadata": {},
   "source": [
    "## 5. CSV File Management\n",
    "\n",
    "Set up CSV file creation and management functions for data storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_csv_file(config):\n",
    "    \"\"\"\n",
    "    Initialize CSV file with headers if it doesn't exist\n",
    "    \n",
    "    Args:\n",
    "        config: CamHRConfig instance\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if file was created/exists, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file already exists\n",
    "        file_exists = os.path.exists(config.CSV_FILENAME)\n",
    "        \n",
    "        if not file_exists:\n",
    "            # Create new CSV file with headers\n",
    "            with open(config.CSV_FILENAME, mode=\"w\", newline=\"\", encoding=\"utf-8-sig\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(config.COLUMNS)\n",
    "            print(f\"‚úÖ Created new CSV file: {config.CSV_FILENAME}\")\n",
    "        else:\n",
    "            print(f\"üìÅ Using existing CSV file: {config.CSV_FILENAME}\")\n",
    "        \n",
    "        print(f\"üìù CSV columns ({len(config.COLUMNS)}): {', '.join(config.COLUMNS)}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing CSV file: {e}\")\n",
    "        return False\n",
    "\n",
    "def write_job_data(config, job_data):\n",
    "    \"\"\"\n",
    "    Write job data to CSV file\n",
    "    \n",
    "    Args:\n",
    "        config: CamHRConfig instance\n",
    "        job_data: Dictionary containing job information\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(config.CSV_FILENAME, mode=\"a\", newline=\"\", encoding=\"utf-8-sig\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            row_data = [job_data.get(col, \"Not found\") for col in config.COLUMNS]\n",
    "            writer.writerow(row_data)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error writing to CSV: {e}\")\n",
    "        return False\n",
    "\n",
    "# Initialize CSV file\n",
    "csv_initialized = initialize_csv_file(config)\n",
    "\n",
    "if csv_initialized:\n",
    "    print(\"üéØ CSV file ready for data storage!\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to initialize CSV file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798458f",
   "metadata": {},
   "source": [
    "## 6. Main Scraping Function\n",
    "\n",
    "Define the main function that orchestrates the scraping process for a single job listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f02fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_job(driver, config, job_id):\n",
    "    \"\"\"\n",
    "    Scrape a single job listing from CamHR\n",
    "    \n",
    "    Args:\n",
    "        driver: WebDriver instance\n",
    "        config: CamHRConfig instance\n",
    "        job_id: Job ID to scrape\n",
    "    \n",
    "    Returns:\n",
    "        dict: Job data dictionary or None if failed\n",
    "    \"\"\"\n",
    "    url = config.BASE_URL.format(job_id)\n",
    "    print(f\"üîç Scraping Job ID {job_id}: {url}\")\n",
    "    \n",
    "    try:\n",
    "        # Navigate to job page\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for page to load\n",
    "        try:\n",
    "            WebDriverWait(driver, config.WAIT_TIMEOUT).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"job-header-content\"))\n",
    "            )\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è Page not loaded properly for job ID {job_id}, skipping...\")\n",
    "            return None\n",
    "        \n",
    "        # Add small delay\n",
    "        time.sleep(config.DELAY)\n",
    "        \n",
    "        # Parse page with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        # Initialize job information dictionary\n",
    "        job_info = {col: \"Not found\" for col in config.COLUMNS}\n",
    "        \n",
    "        # Extract basic information\n",
    "        job_info[\"Job Title\"] = extract_job_title(soup)\n",
    "        job_info[\"Company Name\"] = extract_company_name(soup)\n",
    "        \n",
    "        # Extract table data\n",
    "        table_data = extract_table_data(soup, config.COLUMNS)\n",
    "        job_info.update(table_data)\n",
    "        \n",
    "        # Extract job requirements\n",
    "        job_info[\"Job Requirements\"] = extract_job_requirements(soup)\n",
    "        \n",
    "        # Extract dates\n",
    "        publish_date, closing_date = extract_dates(soup)\n",
    "        job_info[\"Publish Date\"] = publish_date\n",
    "        job_info[\"Closing Date\"] = closing_date\n",
    "        \n",
    "        # Add URL\n",
    "        job_info[\"Link URL\"] = url\n",
    "        \n",
    "        # Print extracted data summary\n",
    "        print(f\"‚úÖ Successfully extracted: {job_info['Job Title']}\")\n",
    "        print(f\"   üè¢ Company: {job_info['Company Name']}\")\n",
    "        print(f\"   üìç Location: {job_info.get('Location', 'Not found')}\")\n",
    "        print(f\"   üí∞ Salary: {job_info.get('Salary', 'Not found')}\")\n",
    "        print(f\"   üìÖ Closing: {job_info['Closing Date']}\")\n",
    "        \n",
    "        return job_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping job ID {job_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Main scraping function defined successfully!\")\n",
    "print(\"üîß Function: scrape_single_job() - Handles complete job data extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45877ea",
   "metadata": {},
   "source": [
    "## 7. Execute the Complete Scraping Process\n",
    "\n",
    "Run the complete scraping process for all job IDs in the specified range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_camhr_scraper(driver, config):\n",
    "    \"\"\"\n",
    "    Execute the complete CamHR scraping process\n",
    "    \n",
    "    Args:\n",
    "        driver: WebDriver instance\n",
    "        config: CamHRConfig instance\n",
    "    \n",
    "    Returns:\n",
    "        dict: Scraping statistics\n",
    "    \"\"\"\n",
    "    if not driver:\n",
    "        print(\"‚ùå WebDriver not available. Please initialize the driver first.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"üöÄ Starting CamHR job scraping process...\")\n",
    "    print(f\"üìä Job ID range: {config.START_ID} to {config.END_ID}\")\n",
    "    print(f\"üìÅ Output file: {config.CSV_FILENAME}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize counters\n",
    "    successful_scrapes = 0\n",
    "    failed_scrapes = 0\n",
    "    total_jobs = config.END_ID - config.START_ID + 1\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Process each job ID\n",
    "        for job_id in range(config.START_ID, config.END_ID + 1):\n",
    "            current_job = job_id - config.START_ID + 1\n",
    "            print(f\"\\nüìä Progress: {current_job}/{total_jobs} ({(current_job/total_jobs)*100:.1f}%)\")\n",
    "            \n",
    "            # Scrape single job\n",
    "            job_data = scrape_single_job(driver, config, job_id)\n",
    "            \n",
    "            if job_data:\n",
    "                # Write to CSV\n",
    "                if write_job_data(config, job_data):\n",
    "                    successful_scrapes += 1\n",
    "                    print(f\"üíæ Data saved to CSV successfully\")\n",
    "                else:\n",
    "                    failed_scrapes += 1\n",
    "                    print(f\"‚ùå Failed to save data to CSV\")\n",
    "            else:\n",
    "                failed_scrapes += 1\n",
    "                print(f\"‚è© Skipped job ID {job_id}\")\n",
    "            \n",
    "            # Calculate and display time estimates\n",
    "            if current_job > 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                avg_time_per_job = elapsed_time / current_job\n",
    "                remaining_jobs = total_jobs - current_job\n",
    "                estimated_time_remaining = avg_time_per_job * remaining_jobs\n",
    "                \n",
    "                print(f\"‚è±Ô∏è Avg time per job: {avg_time_per_job:.2f}s | ETA: {estimated_time_remaining/60:.1f} min\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö†Ô∏è Scraping interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error during scraping process: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Calculate final statistics\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"üéâ Scraping process completed!\")\n",
    "        print(f\"‚úÖ Successful scrapes: {successful_scrapes}\")\n",
    "        print(f\"‚ùå Failed scrapes: {failed_scrapes}\")\n",
    "        print(f\"üìä Success rate: {(successful_scrapes/total_jobs)*100:.1f}%\")\n",
    "        print(f\"‚è±Ô∏è Total time: {total_time/60:.1f} minutes\")\n",
    "        print(f\"‚ö° Average time per job: {total_time/total_jobs:.2f} seconds\")\n",
    "        print(f\"üíæ Data saved to: {config.CSV_FILENAME}\")\n",
    "        \n",
    "        return {\n",
    "            'successful': successful_scrapes,\n",
    "            'failed': failed_scrapes,\n",
    "            'total': total_jobs,\n",
    "            'success_rate': (successful_scrapes/total_jobs)*100,\n",
    "            'total_time': total_time\n",
    "        }\n",
    "\n",
    "# Execute the scraping process (uncomment to run)\n",
    "# scraping_stats = run_camhr_scraper(driver, config)\n",
    "\n",
    "print(\"üîÑ To start scraping, uncomment the 'run_camhr_scraper()' line above and run this cell.\")\n",
    "print(\"‚ö†Ô∏è Warning: This process may take considerable time depending on the number of jobs.\")\n",
    "print(f\"üìä Estimated time for {config.END_ID - config.START_ID + 1} jobs: ~{((config.END_ID - config.START_ID + 1) * 2)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed3b64",
   "metadata": {},
   "source": [
    "## 8. Data Analysis and Insights\n",
    "\n",
    "Analyze the scraped data to gain insights into the job market trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c39765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_camhr_data(config):\n",
    "    \"\"\"\n",
    "    Analyze the scraped CamHR job data\n",
    "    \n",
    "    Args:\n",
    "        config: CamHRConfig instance\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Loaded dataset or None if file not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the CSV data\n",
    "        df = pd.read_csv(config.CSV_FILENAME)\n",
    "        \n",
    "        print(\"üìä CamHR Job Market Analysis\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Basic dataset information\n",
    "        print(f\"üìà Dataset Overview:\")\n",
    "        print(f\"   Total job listings: {len(df)}\")\n",
    "        print(f\"   Data columns: {len(df.columns)}\")\n",
    "        print(f\"   Date range: {df['Publish Date'].min()} to {df['Publish Date'].max()}\")\n",
    "        \n",
    "        # Display first few records\n",
    "        print(f\"\\nüìã Sample Data (First 3 Records):\")\n",
    "        display_cols = ['Job Title', 'Company Name', 'Location', 'Salary', 'Level']\n",
    "        print(df[display_cols].head(3).to_string(index=False))\n",
    "        \n",
    "        # Top companies analysis\n",
    "        print(f\"\\nüè¢ Top Hiring Companies:\")\n",
    "        top_companies = df['Company Name'].value_counts().head(10)\n",
    "        for company, count in top_companies.items():\n",
    "            if company != 'Not found':\n",
    "                print(f\"   {company}: {count} jobs\")\n",
    "        \n",
    "        # Location analysis\n",
    "        print(f\"\\nüåç Top Job Locations:\")\n",
    "        top_locations = df['Location'].value_counts().head(10)\n",
    "        for location, count in top_locations.items():\n",
    "            if location != 'Not found':\n",
    "                print(f\"   {location}: {count} jobs\")\n",
    "        \n",
    "        # Industry analysis\n",
    "        print(f\"\\nüè≠ Top Industries:\")\n",
    "        top_industries = df['Industry'].value_counts().head(10)\n",
    "        for industry, count in top_industries.items():\n",
    "            if industry != 'Not found':\n",
    "                print(f\"   {industry}: {count} jobs\")\n",
    "        \n",
    "        # Job level distribution\n",
    "        print(f\"\\nüìä Job Level Distribution:\")\n",
    "        level_distribution = df['Level'].value_counts()\n",
    "        for level, count in level_distribution.items():\n",
    "            if level != 'Not found':\n",
    "                percentage = (count / len(df)) * 100\n",
    "                print(f\"   {level}: {count} jobs ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Function/Role analysis\n",
    "        print(f\"\\nüéØ Top Job Functions:\")\n",
    "        top_functions = df['Function'].value_counts().head(10)\n",
    "        for function, count in top_functions.items():\n",
    "            if function != 'Not found':\n",
    "                print(f\"   {function}: {count} jobs\")\n",
    "        \n",
    "        # Data quality assessment\n",
    "        print(f\"\\nüîç Data Quality Assessment:\")\n",
    "        for column in config.COLUMNS:\n",
    "            if column in df.columns:\n",
    "                missing_count = (df[column] == 'Not found').sum()\n",
    "                missing_percentage = (missing_count / len(df)) * 100\n",
    "                completeness = 100 - missing_percentage\n",
    "                status = \"‚úÖ\" if completeness > 80 else \"‚ö†Ô∏è\" if completeness > 50 else \"‚ùå\"\n",
    "                print(f\"   {status} {column}: {completeness:.1f}% complete\")\n",
    "        \n",
    "        # Experience requirements analysis\n",
    "        print(f\"\\nüíº Experience Requirements:\")\n",
    "        exp_counts = df['Year of Exp.'].value_counts().head(8)\n",
    "        for exp, count in exp_counts.items():\n",
    "            if exp != 'Not found':\n",
    "                print(f\"   {exp}: {count} jobs\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File {config.CSV_FILENAME} not found. Please run the scraper first.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run data analysis\n",
    "df = analyze_camhr_data(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb4d4d",
   "metadata": {},
   "source": [
    "## 9. Data Export and Advanced Features\n",
    "\n",
    "Export data to multiple formats and provide advanced search capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5504b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data_multiple_formats(config):\n",
    "    \"\"\"\n",
    "    Export scraped data to multiple formats\n",
    "    \n",
    "    Args:\n",
    "        config: CamHRConfig instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the CSV data\n",
    "        df = pd.read_csv(config.CSV_FILENAME)\n",
    "        base_filename = config.CSV_FILENAME.replace('.csv', '')\n",
    "        \n",
    "        print(\"üì¶ Exporting CamHR data to multiple formats...\")\n",
    "        \n",
    "        # Export to Excel with formatting\n",
    "        excel_filename = f\"{base_filename}.xlsx\"\n",
    "        with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name='CamHR_Jobs', index=False)\n",
    "            \n",
    "            # Create summary sheet\n",
    "            summary_data = {\n",
    "                'Metric': ['Total Jobs', 'Unique Companies', 'Unique Locations', 'Date Scraped'],\n",
    "                'Value': [len(df), df['Company Name'].nunique(), df['Location'].nunique(), datetime.now().strftime('%Y-%m-%d')]\n",
    "            }\n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "        \n",
    "        print(f\"‚úÖ Excel export completed: {excel_filename}\")\n",
    "        \n",
    "        # Export to JSON\n",
    "        json_filename = f\"{base_filename}.json\"\n",
    "        df.to_json(json_filename, orient='records', indent=2)\n",
    "        print(f\"‚úÖ JSON export completed: {json_filename}\")\n",
    "        \n",
    "        # Create cleaned dataset (remove 'Not found' entries)\n",
    "        df_cleaned = df.replace('Not found', '')\n",
    "        cleaned_filename = f\"{base_filename}_cleaned.csv\"\n",
    "        df_cleaned.to_csv(cleaned_filename, index=False)\n",
    "        print(f\"‚úÖ Cleaned dataset created: {cleaned_filename}\")\n",
    "        \n",
    "        # Create industry-specific datasets\n",
    "        if 'Industry' in df.columns:\n",
    "            top_industries = df['Industry'].value_counts().head(5).index\n",
    "            for industry in top_industries:\n",
    "                if industry != 'Not found':\n",
    "                    industry_df = df[df['Industry'] == industry]\n",
    "                    industry_filename = f\"{base_filename}_{industry.replace(' ', '_').replace('/', '_')}.csv\"\n",
    "                    industry_df.to_csv(industry_filename, index=False)\n",
    "                    print(f\"üìä Industry dataset created: {industry_filename} ({len(industry_df)} jobs)\")\n",
    "        \n",
    "        print(f\"\\nüìÅ Export Summary:\")\n",
    "        print(f\"   üìä Original CSV: {config.CSV_FILENAME}\")\n",
    "        print(f\"   üìà Excel file: {excel_filename}\")\n",
    "        print(f\"   üîó JSON file: {json_filename}\")\n",
    "        print(f\"   üßπ Cleaned CSV: {cleaned_filename}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File {config.CSV_FILENAME} not found. Please run the scraper first.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exporting data: {e}\")\n",
    "\n",
    "def search_camhr_jobs(config, **filters):\n",
    "    \"\"\"\n",
    "    Search and filter CamHR jobs based on criteria\n",
    "    \n",
    "    Args:\n",
    "        config: CamHRConfig instance\n",
    "        **filters: Keyword arguments for filtering (title, company, location, level, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Filtered job data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(config.CSV_FILENAME)\n",
    "        filtered_df = df.copy()\n",
    "        \n",
    "        print(f\"üîç Searching CamHR jobs with filters:\")\n",
    "        \n",
    "        # Apply filters\n",
    "        for key, value in filters.items():\n",
    "            if value and key in df.columns:\n",
    "                filtered_df = filtered_df[filtered_df[key].str.contains(str(value), case=False, na=False)]\n",
    "                print(f\"   üìù {key}: '{value}'\")\n",
    "        \n",
    "        print(f\"\\nüìä Search Results: {len(filtered_df)} jobs found\")\n",
    "        \n",
    "        if len(filtered_df) > 0:\n",
    "            print(f\"\\nüìã Results Preview:\")\n",
    "            display_cols = ['Job Title', 'Company Name', 'Location', 'Level', 'Salary']\n",
    "            print(filtered_df[display_cols].head(5).to_string(index=False))\n",
    "        \n",
    "        return filtered_df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File {config.CSV_FILENAME} not found. Please run the scraper first.\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error searching jobs: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Export data to multiple formats\n",
    "export_data_multiple_formats(config)\n",
    "\n",
    "print(\"\\nüîç Search Examples:\")\n",
    "print(\"# Search for manager positions:\")\n",
    "print(\"# results = search_camhr_jobs(config, **{'Job Title': 'manager'})\")\n",
    "print(\"\\n# Search for IT jobs in specific location:\")\n",
    "print(\"# results = search_camhr_jobs(config, **{'Function': 'IT', 'Location': 'Phnom Penh'})\")\n",
    "print(\"\\n# Search for senior level positions:\")\n",
    "print(\"# results = search_camhr_jobs(config, **{'Level': 'Senior'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6a798e",
   "metadata": {},
   "source": [
    "## 10. Cleanup and Final Steps\n",
    "\n",
    "Properly close the WebDriver and provide final summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7efc41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_resources(driver):\n",
    "    \"\"\"\n",
    "    Clean up resources and close WebDriver\n",
    "    \n",
    "    Args:\n",
    "        driver: WebDriver instance to close\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "            print(\"‚úÖ WebDriver closed successfully\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è No WebDriver to close\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error closing WebDriver: {e}\")\n",
    "\n",
    "def generate_scraping_report(config):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive scraping report\n",
    "    \n",
    "    Args:\n",
    "        config: CamHRConfig instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        report_filename = config.CSV_FILENAME.replace('.csv', '_report.txt')\n",
    "        \n",
    "        with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"CamHR Job Scraping Report\\n\")\n",
    "            f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "            f.write(f\"Scraping Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Source Website: CamHR.com\\n\")\n",
    "            f.write(f\"Job ID Range: {config.START_ID} to {config.END_ID}\\n\")\n",
    "            f.write(f\"Base URL: {config.BASE_URL}\\n\")\n",
    "            f.write(f\"Output File: {config.CSV_FILENAME}\\n\\n\")\n",
    "            \n",
    "            # Add data fields information\n",
    "            f.write(\"Data Fields Extracted:\\n\")\n",
    "            for i, column in enumerate(config.COLUMNS, 1):\n",
    "                f.write(f\"  {i:2d}. {column}\\n\")\n",
    "            \n",
    "            f.write(\"\\nConfiguration Settings:\\n\")\n",
    "            f.write(f\"  Wait Timeout: {config.WAIT_TIMEOUT} seconds\\n\")\n",
    "            f.write(f\"  Request Delay: {config.DELAY} seconds\\n\")\n",
    "            f.write(f\"  Chrome Driver: {config.CHROME_DRIVER_PATH}\\n\")\n",
    "        \n",
    "        print(f\"üìã Scraping report generated: {report_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating report: {e}\")\n",
    "\n",
    "# Generate final report\n",
    "generate_scraping_report(config)\n",
    "\n",
    "# Clean up resources (uncomment when done with scraping)\n",
    "# cleanup_resources(driver)\n",
    "\n",
    "print(\"\\nüéØ CamHR Scraper Setup Complete!\")\n",
    "print(\"üìã Next Steps:\")\n",
    "print(\"   1. Run the scraping process by uncommenting 'run_camhr_scraper()'\")\n",
    "print(\"   2. Analyze results using the analysis functions\")\n",
    "print(\"   3. Export data to different formats as needed\")\n",
    "print(\"   4. Clean up resources when finished\")\n",
    "print(\"\\n‚ö†Ô∏è Remember to uncomment 'cleanup_resources(driver)' when done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73abe6a",
   "metadata": {},
   "source": [
    "## 11. Summary and Best Practices\n",
    "\n",
    "This comprehensive Jupyter notebook provides a complete solution for scraping job data from CamHR.com.\n",
    "\n",
    "### üéØ Key Features Implemented:\n",
    "\n",
    "1. **üîß Professional Configuration**: Centralized configuration class for easy customization\n",
    "2. **üåê Optimized WebDriver**: Headless Chrome setup with performance optimizations\n",
    "3. **üìä Comprehensive Data Extraction**: 18 different job attributes extracted\n",
    "4. **üõ°Ô∏è Robust Error Handling**: Graceful handling of missing elements and failed requests\n",
    "5. **üíæ Multiple Export Formats**: CSV, Excel, JSON, and industry-specific datasets\n",
    "6. **üìà Advanced Analytics**: Built-in analysis and market insights\n",
    "7. **üîç Search Functionality**: Advanced filtering and search capabilities\n",
    "8. **üìã Progress Tracking**: Real-time updates and performance metrics\n",
    "9. **üßπ Resource Management**: Proper cleanup and memory management\n",
    "10. **üìù Documentation**: Comprehensive reports and documentation\n",
    "\n",
    "### üìä Data Fields Extracted:\n",
    "\n",
    "- **Basic Information**: Job Title, Company Name, Link URL\n",
    "- **Job Classification**: Level, Function, Industry\n",
    "- **Requirements**: Years of Experience, Qualification, Language\n",
    "- **Employment Details**: Hiring status, Salary, Employment Term\n",
    "- **Demographics**: Sex, Age requirements\n",
    "- **Location**: Job location information\n",
    "- **Detailed Requirements**: Comprehensive job requirements\n",
    "- **Timeline**: Publish Date, Closing Date\n",
    "\n",
    "### üöÄ Usage Instructions:\n",
    "\n",
    "1. **Configuration**: Modify `CamHRConfig` class parameters as needed\n",
    "2. **Execution**: Run cells sequentially, uncomment scraper execution\n",
    "3. **Monitoring**: Track progress through real-time updates\n",
    "4. **Analysis**: Use built-in analysis tools to examine results\n",
    "5. **Export**: Generate multiple output formats for further use\n",
    "6. **Cleanup**: Properly close resources when finished\n",
    "\n",
    "### ‚ö° Performance Features:\n",
    "\n",
    "- **Headless Mode**: Faster execution without GUI\n",
    "- **Efficient Selectors**: Optimized CSS selectors and XPath\n",
    "- **Smart Waiting**: Intelligent wait conditions for page loads\n",
    "- **Memory Management**: Proper resource cleanup\n",
    "- **Progress Estimation**: ETA calculations for long-running processes\n",
    "\n",
    "### üîç Advanced Analytics:\n",
    "\n",
    "- **Market Insights**: Top companies, locations, and industries\n",
    "- **Trend Analysis**: Job level and function distributions\n",
    "- **Data Quality**: Completeness metrics for each field\n",
    "- **Search Capabilities**: Multi-criteria filtering and search\n",
    "\n",
    "### üìÅ Export Options:\n",
    "\n",
    "- **CSV**: Original structured data\n",
    "- **Excel**: Formatted spreadsheet with summary\n",
    "- **JSON**: API-friendly format\n",
    "- **Cleaned Data**: Processed datasets without missing values\n",
    "- **Industry-Specific**: Segmented datasets by industry\n",
    "\n",
    "### ‚öñÔ∏è Ethical Considerations:\n",
    "\n",
    "- ‚úÖ **Respectful Delays**: Built-in delays between requests\n",
    "- ‚úÖ **Error Handling**: Graceful handling of failures\n",
    "- ‚úÖ **Rate Limiting**: Controlled request frequency\n",
    "- ‚úÖ **Resource Management**: Proper cleanup of browser resources\n",
    "- ‚ö†Ô∏è **Terms of Service**: Always check website's ToS before scraping\n",
    "- ‚ö†Ô∏è **Server Load**: Monitor response times and adjust delays\n",
    "\n",
    "### üîß Troubleshooting Tips:\n",
    "\n",
    "1. **WebDriver Issues**: Ensure Chrome WebDriver path is correct\n",
    "2. **Timeout Errors**: Increase `WAIT_TIMEOUT` for slower connections\n",
    "3. **Missing Data**: Check website structure changes\n",
    "4. **Memory Issues**: Process data in smaller batches\n",
    "5. **Network Issues**: Add retry mechanisms for failed requests\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Job Market Analysis! üöÄ**\n",
    "\n",
    "*This scraper was designed for educational and research purposes. Please ensure compliance with CamHR.com's terms of service and respect their server resources.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
