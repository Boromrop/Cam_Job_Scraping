{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a95ab6",
   "metadata": {},
   "source": [
    "# Jobify.works Job Scraper\n",
    "\n",
    "This notebook contains a comprehensive web scraper for extracting job listings from Jobify.works. The scraper systematically collects detailed job information including titles, salaries, requirements, qualifications, and other essential job details.\n",
    "\n",
    "## Features\n",
    "- **Automated Web Scraping**: Uses Selenium WebDriver for dynamic content extraction\n",
    "- **Comprehensive Data Collection**: Extracts 16 different job attributes\n",
    "- **Error Handling**: Robust error handling for missing elements\n",
    "- **CSV Export**: Saves collected data in structured CSV format\n",
    "- **Progress Tracking**: Real-time progress updates during scraping\n",
    "- **Headless Operation**: Runs in background for better performance\n",
    "\n",
    "## Data Fields Extracted\n",
    "- Job Title\n",
    "- Job Link\n",
    "- Salary\n",
    "- Job Type\n",
    "- Job Level\n",
    "- Gender Requirements\n",
    "- Age Requirements\n",
    "- Years of Experience\n",
    "- Language Requirements\n",
    "- Category\n",
    "- Industry\n",
    "- Location\n",
    "- Qualification\n",
    "- Available Positions\n",
    "- Required Skills\n",
    "- Job Requirements\n",
    "\n",
    "## Requirements\n",
    "- Python 3.x\n",
    "- Selenium WebDriver\n",
    "- Chrome WebDriver\n",
    "- CSV module (built-in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038d921",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for web scraping and data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Scraping session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2239660",
   "metadata": {},
   "source": [
    "## 2. Configuration Settings\n",
    "\n",
    "Configure all the necessary settings for the web scraper including file paths, output settings, and scraping parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Settings\n",
    "class JobifyConfig:\n",
    "    # Path to Chrome WebDriver executable\n",
    "    CHROME_DRIVER_PATH = r\"D:\\DSE_Folder\\Year_3\\Sem_2\\Web Scraping\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "    \n",
    "    # Output CSV filename\n",
    "    OUTPUT_FILENAME = \"job4.csv\"\n",
    "    \n",
    "    # Scraping range settings\n",
    "    START_ID = 1086  # Starting job ID\n",
    "    END_ID = 500     # Ending job ID\n",
    "    STEP = -1        # Step direction (negative for reverse)\n",
    "    \n",
    "    # Base URL template for job pages\n",
    "    BASE_URL = \"https://jobify.works/jobs/{}\"\n",
    "    \n",
    "    # Timing configurations\n",
    "    WAIT_TIMEOUT = 10  # seconds to wait for page elements\n",
    "    \n",
    "    # CSV column headers\n",
    "    CSV_HEADERS = [\n",
    "        \"Job Title\", \"Job Link\", \"Salary\", \"Job Type\", \"Job Level\", \"Gender\", \"Age\",\n",
    "        \"Years of Experience\", \"Language\", \"Category\", \"Industry\", \"Location\", \"Qualification\",\n",
    "        \"Available Position\", \"Required Skills\", \"Job Requirement\"\n",
    "    ]\n",
    "\n",
    "# Display configuration\n",
    "config = JobifyConfig()\n",
    "print(\"üîß Configuration Settings:\")\n",
    "print(f\"üìä Scraping range: {config.START_ID} to {config.END_ID} (step: {config.STEP})\")\n",
    "print(f\"üíæ Output file: {config.OUTPUT_FILENAME}\")\n",
    "print(f\"‚è±Ô∏è Wait timeout: {config.WAIT_TIMEOUT} seconds\")\n",
    "print(f\"üìù Total fields to extract: {len(config.CSV_HEADERS)}\")\n",
    "print(f\"üîó Base URL: {config.BASE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc332de3",
   "metadata": {},
   "source": [
    "## 3. WebDriver Setup\n",
    "\n",
    "Initialize the Chrome WebDriver with optimized settings for web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c30a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_webdriver(chrome_driver_path):\n",
    "    \"\"\"\n",
    "    Initialize Chrome WebDriver with optimized settings\n",
    "    \n",
    "    Args:\n",
    "        chrome_driver_path (str): Path to Chrome WebDriver executable\n",
    "    \n",
    "    Returns:\n",
    "        webdriver.Chrome: Configured Chrome WebDriver instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configure Chrome service\n",
    "        service = Service(chrome_driver_path)\n",
    "        \n",
    "        # Configure Chrome options\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--headless\")              # Run in background\n",
    "        options.add_argument(\"--disable-gpu\")           # Disable GPU acceleration\n",
    "        options.add_argument(\"--no-sandbox\")            # Bypass OS security model\n",
    "        options.add_argument(\"--disable-dev-shm-usage\") # Overcome limited resource problems\n",
    "        options.add_argument(\"--window-size=1920,1080\")  # Set window size\n",
    "        \n",
    "        # Initialize WebDriver\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        \n",
    "        print(\"‚úÖ Chrome WebDriver initialized successfully!\")\n",
    "        print(f\"üåê Browser version: {driver.capabilities['browserVersion']}\")\n",
    "        print(f\"üîß Driver version: {driver.capabilities['chrome']['chromedriverVersion'].split(' ')[0]}\")\n",
    "        \n",
    "        return driver\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing WebDriver: {e}\")\n",
    "        return None\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = setup_webdriver(config.CHROME_DRIVER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d6c73",
   "metadata": {},
   "source": [
    "## 4. Data Extraction Functions\n",
    "\n",
    "Define helper functions for extracting specific job details from the web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4da1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_title(driver):\n",
    "    \"\"\"\n",
    "    Extract job title from the page\n",
    "    \n",
    "    Args:\n",
    "        driver: WebDriver instance\n",
    "    \n",
    "    Returns:\n",
    "        str: Job title or 'N/A' if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        title_element = WebDriverWait(driver, config.WAIT_TIMEOUT).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"job-title\"))\n",
    "        )\n",
    "        return title_element.text.strip() if title_element.text.strip() else \"N/A\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting job title: {e}\")\n",
    "        return \"N/A\"\n",
    "\n",
    "def get_job_detail(driver, label):\n",
    "    \"\"\"\n",
    "    Extract specific job detail using label-based XPath\n",
    "    \n",
    "    Args:\n",
    "        driver: WebDriver instance\n",
    "        label (str): Label text to search for (e.g., 'Salary:', 'Job Type:')\n",
    "    \n",
    "    Returns:\n",
    "        str: Job detail value or 'N/A' if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        element = driver.find_element(By.XPATH, f\"//strong[text()='{label}']\")\n",
    "        return element.find_element(By.XPATH, \"./following-sibling::text()\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting {label}: {e}\")\n",
    "        return \"N/A\"\n",
    "\n",
    "def extract_job_requirements(driver, url):\n",
    "    \"\"\"\n",
    "    Extract job requirements from the dedicated section\n",
    "    \n",
    "    Args:\n",
    "        driver: WebDriver instance\n",
    "        url (str): Current page URL for error reporting\n",
    "    \n",
    "    Returns:\n",
    "        str: Job requirements or 'N/A' if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Wait for job requirement section to load\n",
    "        job_req_section = WebDriverWait(driver, config.WAIT_TIMEOUT).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//h5[text()='Job Requirement']/following-sibling::div\"))\n",
    "        )\n",
    "        \n",
    "        # Extract all list items from unordered lists\n",
    "        ul_elements = job_req_section.find_elements(By.TAG_NAME, \"ul\")\n",
    "        li_elements = [\n",
    "            li.text.strip() \n",
    "            for ul in ul_elements \n",
    "            for li in ul.find_elements(By.TAG_NAME, \"li\") \n",
    "            if li.text.strip()\n",
    "        ]\n",
    "        \n",
    "        return \" | \".join(li_elements) if li_elements else \"N/A\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Job Requirement not found for {url}: {e}\")\n",
    "        return \"N/A\"\n",
    "\n",
    "print(\"‚úÖ Data extraction functions defined successfully!\")\n",
    "print(\"üîß Functions available:\")\n",
    "print(\"   - extract_job_title(): Extracts job title\")\n",
    "print(\"   - get_job_detail(): Extracts labeled job details\")\n",
    "print(\"   - extract_job_requirements(): Extracts job requirements list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14710ec0",
   "metadata": {},
   "source": [
    "## 5. Main Scraping Function\n",
    "\n",
    "Define the main function that orchestrates the entire scraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46603519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_job(driver, job_id):\n",
    "    \"\"\"\n",
    "    Scrape a single job listing\n",
    "    \n",
    "    Args:\n",
    "        driver: WebDriver instance\n",
    "        job_id (int): Job ID to scrape\n",
    "    \n",
    "    Returns:\n",
    "        list: List of extracted job data or None if failed\n",
    "    \"\"\"\n",
    "    url = config.BASE_URL.format(job_id)\n",
    "    print(f\"üîç Fetching Job ID {job_id}: {url}\")\n",
    "    \n",
    "    try:\n",
    "        # Navigate to job page\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Extract job title first (acts as a page load indicator)\n",
    "        title = extract_job_title(driver)\n",
    "        \n",
    "        if title == \"N/A\":\n",
    "            print(f\"‚ö†Ô∏è Job title not found for ID {job_id}, skipping...\")\n",
    "            return None\n",
    "        \n",
    "        # Extract all labeled job details\n",
    "        job_details = {\n",
    "            'salary': get_job_detail(driver, \"Salary:\"),\n",
    "            'job_type': get_job_detail(driver, \"Job Type:\"),\n",
    "            'job_level': get_job_detail(driver, \"Job Level:\"),\n",
    "            'gender': get_job_detail(driver, \"Gender:\"),\n",
    "            'age': get_job_detail(driver, \"Age:\"),\n",
    "            'experience': get_job_detail(driver, \"Years of Experience:\"),\n",
    "            'language': get_job_detail(driver, \"Language:\"),\n",
    "            'category': get_job_detail(driver, \"Category:\"),\n",
    "            'industry': get_job_detail(driver, \"Industry:\"),\n",
    "            'location': get_job_detail(driver, \"Location:\"),\n",
    "            'qualification': get_job_detail(driver, \"Qualification:\"),\n",
    "            'available_position': get_job_detail(driver, \"Available Position:\"),\n",
    "            'required_skills': get_job_detail(driver, \"Required Skills:\")\n",
    "        }\n",
    "        \n",
    "        # Extract job requirements\n",
    "        job_requirement = extract_job_requirements(driver, url)\n",
    "        \n",
    "        # Compile all data\n",
    "        job_data = [\n",
    "            title, url, job_details['salary'], job_details['job_type'], \n",
    "            job_details['job_level'], job_details['gender'], job_details['age'],\n",
    "            job_details['experience'], job_details['language'], job_details['category'],\n",
    "            job_details['industry'], job_details['location'], job_details['qualification'],\n",
    "            job_details['available_position'], job_details['required_skills'], job_requirement\n",
    "        ]\n",
    "        \n",
    "        print(f\"‚úÖ Successfully extracted: {title}\")\n",
    "        print(f\"   üìç Location: {job_details['location']}\")\n",
    "        print(f\"   üí∞ Salary: {job_details['salary']}\")\n",
    "        print(f\"   üè¢ Company Type: {job_details['job_type']}\")\n",
    "        \n",
    "        return job_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping job ID {job_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Main scraping function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba76586f",
   "metadata": {},
   "source": [
    "## 6. Execute the Scraping Process\n",
    "\n",
    "Now let's run the complete scraping process. This cell will iterate through all job IDs and save the data to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcafc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_jobify_scraper():\n",
    "    \"\"\"\n",
    "    Execute the complete Jobify scraping process\n",
    "    \"\"\"\n",
    "    if driver is None:\n",
    "        print(\"‚ùå WebDriver not initialized. Please run the WebDriver setup cell first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"üöÄ Starting Jobify.works scraping process...\")\n",
    "    print(f\"üìä Range: {config.START_ID} to {config.END_ID} (step: {config.STEP})\")\n",
    "    print(f\"üìÅ Output file: {config.OUTPUT_FILENAME}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize counters\n",
    "    successful_scrapes = 0\n",
    "    failed_scrapes = 0\n",
    "    total_jobs = abs(config.START_ID - config.END_ID) + 1\n",
    "    \n",
    "    # Open CSV file for writing\n",
    "    try:\n",
    "        with open(config.OUTPUT_FILENAME, \"w\", encoding=\"utf-8\", newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            \n",
    "            # Write CSV headers\n",
    "            writer.writerow(config.CSV_HEADERS)\n",
    "            print(f\"üìù CSV file created with headers: {len(config.CSV_HEADERS)} columns\")\n",
    "            \n",
    "            # Loop through job IDs\n",
    "            for job_id in range(config.START_ID, config.END_ID + config.STEP, config.STEP):\n",
    "                print(f\"\\nüìä Progress: {successful_scrapes + failed_scrapes + 1}/{total_jobs}\")\n",
    "                \n",
    "                # Scrape single job\n",
    "                job_data = scrape_single_job(driver, job_id)\n",
    "                \n",
    "                if job_data:\n",
    "                    # Write to CSV\n",
    "                    writer.writerow(job_data)\n",
    "                    successful_scrapes += 1\n",
    "                    print(f\"üíæ Data saved to CSV\")\n",
    "                else:\n",
    "                    failed_scrapes += 1\n",
    "                    print(f\"‚è© Skipped job ID {job_id}\")\n",
    "                \n",
    "                # Add small delay to be respectful to the server\n",
    "                time.sleep(0.5)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üéâ Scraping completed successfully!\")\n",
    "        print(f\"‚úÖ Successful scrapes: {successful_scrapes}\")\n",
    "        print(f\"‚ùå Failed scrapes: {failed_scrapes}\")\n",
    "        print(f\"üìä Success rate: {(successful_scrapes/total_jobs)*100:.1f}%\")\n",
    "        print(f\"üíæ Data saved to: {config.OUTPUT_FILENAME}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during scraping process: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Close the browser\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "            print(\"üîí Browser closed successfully\")\n",
    "\n",
    "# Run the scraper (uncomment the line below to start)\n",
    "# run_jobify_scraper()\n",
    "\n",
    "print(\"üîÑ To start scraping, uncomment the 'run_jobify_scraper()' line above and run this cell.\")\n",
    "print(\"‚ö†Ô∏è Warning: This process may take several minutes depending on the number of jobs to scrape.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48195b49",
   "metadata": {},
   "source": [
    "## 7. Data Analysis and Visualization\n",
    "\n",
    "After scraping is complete, let's analyze the collected data to gain insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_scraped_data():\n",
    "    \"\"\"\n",
    "    Analyze the scraped job data and provide insights\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the CSV data\n",
    "        df = pd.read_csv(config.OUTPUT_FILENAME)\n",
    "        \n",
    "        print(\"üìä Jobify.works Data Analysis\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Basic statistics\n",
    "        print(f\"üìà Dataset Overview:\")\n",
    "        print(f\"   Total jobs scraped: {len(df)}\")\n",
    "        print(f\"   Total columns: {len(df.columns)}\")\n",
    "        print(f\"   Data types: {df.dtypes.value_counts().to_dict()}\")\n",
    "        \n",
    "        # Display first few rows\n",
    "        print(f\"\\nüìã First 5 Records:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Job categories analysis\n",
    "        print(f\"\\nüè∑Ô∏è Top Job Categories:\")\n",
    "        category_counts = df['Category'].value_counts().head(10)\n",
    "        for category, count in category_counts.items():\n",
    "            if category != 'N/A':\n",
    "                print(f\"   {category}: {count} jobs\")\n",
    "        \n",
    "        # Location analysis\n",
    "        print(f\"\\nüåç Top Job Locations:\")\n",
    "        location_counts = df['Location'].value_counts().head(10)\n",
    "        for location, count in location_counts.items():\n",
    "            if location != 'N/A':\n",
    "                print(f\"   {location}: {count} jobs\")\n",
    "        \n",
    "        # Industry analysis\n",
    "        print(f\"\\nüè≠ Top Industries:\")\n",
    "        industry_counts = df['Industry'].value_counts().head(10)\n",
    "        for industry, count in industry_counts.items():\n",
    "            if industry != 'N/A':\n",
    "                print(f\"   {industry}: {count} jobs\")\n",
    "        \n",
    "        # Job level analysis\n",
    "        print(f\"\\nüìä Job Level Distribution:\")\n",
    "        level_counts = df['Job Level'].value_counts()\n",
    "        for level, count in level_counts.items():\n",
    "            if level != 'N/A':\n",
    "                percentage = (count / len(df)) * 100\n",
    "                print(f\"   {level}: {count} jobs ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Data quality analysis\n",
    "        print(f\"\\nüîç Data Quality Analysis:\")\n",
    "        for column in config.CSV_HEADERS:\n",
    "            na_count = (df[column] == 'N/A').sum()\n",
    "            na_percentage = (na_count / len(df)) * 100\n",
    "            print(f\"   {column}: {na_count} missing ({na_percentage:.1f}%)\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File {config.OUTPUT_FILENAME} not found. Please run the scraper first.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run analysis (will only work after scraping is complete)\n",
    "df = analyze_scraped_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a38994",
   "metadata": {},
   "source": [
    "## 8. Data Export and Additional Formats\n",
    "\n",
    "Export the scraped data to different formats for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320723a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data_to_formats():\n",
    "    \"\"\"\n",
    "    Export scraped data to multiple formats\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the CSV data\n",
    "        df = pd.read_csv(config.OUTPUT_FILENAME)\n",
    "        \n",
    "        print(\"üì¶ Exporting data to multiple formats...\")\n",
    "        \n",
    "        # Export to Excel\n",
    "        excel_filename = config.OUTPUT_FILENAME.replace('.csv', '.xlsx')\n",
    "        df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "        print(f\"‚úÖ Excel export completed: {excel_filename}\")\n",
    "        \n",
    "        # Export to JSON\n",
    "        json_filename = config.OUTPUT_FILENAME.replace('.csv', '.json')\n",
    "        df.to_json(json_filename, orient='records', indent=2)\n",
    "        print(f\"‚úÖ JSON export completed: {json_filename}\")\n",
    "        \n",
    "        # Create a cleaned dataset (remove N/A values)\n",
    "        df_cleaned = df.replace('N/A', '')\n",
    "        cleaned_filename = config.OUTPUT_FILENAME.replace('.csv', '_cleaned.csv')\n",
    "        df_cleaned.to_csv(cleaned_filename, index=False)\n",
    "        print(f\"‚úÖ Cleaned CSV export completed: {cleaned_filename}\")\n",
    "        \n",
    "        # Generate summary statistics\n",
    "        summary_filename = config.OUTPUT_FILENAME.replace('.csv', '_summary.txt')\n",
    "        with open(summary_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"Jobify.works Scraping Summary\\n\")\n",
    "            f.write(\"=\" * 40 + \"\\n\")\n",
    "            f.write(f\"Total records: {len(df)}\\n\")\n",
    "            f.write(f\"Scraping date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Source: {config.BASE_URL}\\n\")\n",
    "            f.write(f\"ID range: {config.START_ID} to {config.END_ID}\\n\\n\")\n",
    "            \n",
    "            f.write(\"Column completeness:\\n\")\n",
    "            for column in config.CSV_HEADERS:\n",
    "                na_count = (df[column] == 'N/A').sum()\n",
    "                completeness = ((len(df) - na_count) / len(df)) * 100\n",
    "                f.write(f\"  {column}: {completeness:.1f}% complete\\n\")\n",
    "        \n",
    "        print(f\"‚úÖ Summary report generated: {summary_filename}\")\n",
    "        \n",
    "        print(f\"\\nüìä Export Summary:\")\n",
    "        print(f\"   üìÅ Original CSV: {config.OUTPUT_FILENAME}\")\n",
    "        print(f\"   üìä Excel file: {excel_filename}\")\n",
    "        print(f\"   üîó JSON file: {json_filename}\")\n",
    "        print(f\"   üßπ Cleaned CSV: {cleaned_filename}\")\n",
    "        print(f\"   üìã Summary report: {summary_filename}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File {config.OUTPUT_FILENAME} not found. Please run the scraper first.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exporting data: {e}\")\n",
    "\n",
    "# Export to multiple formats\n",
    "export_data_to_formats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262add0",
   "metadata": {},
   "source": [
    "## 9. Advanced Data Filtering and Search\n",
    "\n",
    "Provide tools for filtering and searching through the scraped job data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_jobs(keyword=None, location=None, category=None, job_level=None, min_salary=None):\n",
    "    \"\"\"\n",
    "    Search and filter jobs based on various criteria\n",
    "    \n",
    "    Args:\n",
    "        keyword (str): Keyword to search in job title\n",
    "        location (str): Job location\n",
    "        category (str): Job category\n",
    "        job_level (str): Job level (Entry, Mid, Senior, etc.)\n",
    "        min_salary (str): Minimum salary filter\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Filtered job data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(config.OUTPUT_FILENAME)\n",
    "        filtered_df = df.copy()\n",
    "        \n",
    "        print(f\"üîç Searching jobs with criteria:\")\n",
    "        \n",
    "        # Apply filters\n",
    "        if keyword:\n",
    "            filtered_df = filtered_df[filtered_df['Job Title'].str.contains(keyword, case=False, na=False)]\n",
    "            print(f\"   üìù Keyword: '{keyword}'\")\n",
    "        \n",
    "        if location:\n",
    "            filtered_df = filtered_df[filtered_df['Location'].str.contains(location, case=False, na=False)]\n",
    "            print(f\"   üìç Location: '{location}'\")\n",
    "        \n",
    "        if category:\n",
    "            filtered_df = filtered_df[filtered_df['Category'].str.contains(category, case=False, na=False)]\n",
    "            print(f\"   üè∑Ô∏è Category: '{category}'\")\n",
    "        \n",
    "        if job_level:\n",
    "            filtered_df = filtered_df[filtered_df['Job Level'].str.contains(job_level, case=False, na=False)]\n",
    "            print(f\"   üìä Job Level: '{job_level}'\")\n",
    "        \n",
    "        print(f\"\\nüìä Search Results: {len(filtered_df)} jobs found\")\n",
    "        \n",
    "        if len(filtered_df) > 0:\n",
    "            print(f\"\\nüìã Sample Results:\")\n",
    "            display_columns = ['Job Title', 'Location', 'Category', 'Job Level', 'Salary']\n",
    "            print(filtered_df[display_columns].head(10).to_string(index=False))\n",
    "        \n",
    "        return filtered_df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File {config.OUTPUT_FILENAME} not found. Please run the scraper first.\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error searching jobs: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example searches (uncomment to try)\n",
    "print(\"üîç Job Search Examples:\")\n",
    "print(\"Uncomment any of the following lines to search for specific jobs:\")\n",
    "print(\"# search_jobs(keyword='developer')\")\n",
    "print(\"# search_jobs(location='Phnom Penh')\")\n",
    "print(\"# search_jobs(category='IT')\")\n",
    "print(\"# search_jobs(job_level='Senior')\")\n",
    "print(\"# search_jobs(keyword='manager', location='Cambodia')\")\n",
    "\n",
    "# Example: Search for developer jobs\n",
    "# results = search_jobs(keyword='developer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f5299",
   "metadata": {},
   "source": [
    "## 10. Summary and Best Practices\n",
    "\n",
    "This notebook provides a comprehensive solution for scraping job data from Jobify.works.\n",
    "\n",
    "### Key Features Implemented:\n",
    "\n",
    "1. **üîß Robust Configuration**: Centralized configuration class for easy customization\n",
    "2. **üåê Optimized WebDriver**: Headless Chrome setup with performance optimizations\n",
    "3. **üìä Comprehensive Data Extraction**: 16 different job attributes extracted\n",
    "4. **üõ°Ô∏è Error Handling**: Graceful handling of missing elements and failed requests\n",
    "5. **üíæ Multiple Export Formats**: CSV, Excel, JSON, and cleaned datasets\n",
    "6. **üìà Data Analysis**: Built-in analysis and visualization tools\n",
    "7. **üîç Search Functionality**: Advanced filtering and search capabilities\n",
    "8. **üìã Progress Tracking**: Real-time updates and success rate monitoring\n",
    "\n",
    "### Data Fields Extracted:\n",
    "\n",
    "- **Basic Info**: Job Title, Job Link, Location\n",
    "- **Employment Details**: Job Type, Job Level, Available Positions\n",
    "- **Requirements**: Years of Experience, Qualification, Required Skills\n",
    "- **Compensation**: Salary information\n",
    "- **Demographics**: Gender, Age requirements\n",
    "- **Classification**: Category, Industry\n",
    "- **Other**: Language requirements, Job Requirements\n",
    "\n",
    "### Usage Instructions:\n",
    "\n",
    "1. **Setup**: Ensure Chrome WebDriver path is correct\n",
    "2. **Configure**: Modify `JobifyConfig` class parameters as needed\n",
    "3. **Execute**: Run cells sequentially, uncomment scraper execution\n",
    "4. **Analyze**: Use built-in analysis tools to examine results\n",
    "5. **Export**: Generate multiple output formats for further use\n",
    "\n",
    "### Ethical Considerations:\n",
    "\n",
    "- ‚úÖ **Respectful Delays**: Built-in delays between requests\n",
    "- ‚úÖ **Error Handling**: Graceful handling of failures\n",
    "- ‚úÖ **Rate Limiting**: Controlled request frequency\n",
    "- ‚ö†Ô∏è **Terms of Service**: Always check website's ToS before scraping\n",
    "- ‚ö†Ô∏è **Server Load**: Monitor server response and adjust delays if needed\n",
    "\n",
    "### Performance Optimization:\n",
    "\n",
    "- **Headless Mode**: Faster execution without GUI\n",
    "- **Efficient Selectors**: Optimized XPath and CSS selectors\n",
    "- **Error Recovery**: Retry mechanisms for failed requests\n",
    "- **Memory Management**: Proper WebDriver cleanup\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Job Hunting! üöÄ**\n",
    "\n",
    "*This scraper was designed for educational and research purposes. Please ensure compliance with the website's terms of service and robots.txt before large-scale usage.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
