{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be46974",
   "metadata": {},
   "source": [
    "# Workinga.com Job Scraper\n",
    "\n",
    "This notebook contains a web scraper for extracting job listings from Workinga.com. The scraper systematically collects job information including titles, company names, salaries, requirements, and other relevant details.\n",
    "\n",
    "## Features\n",
    "- **Automated Web Scraping**: Uses Selenium WebDriver for dynamic content extraction\n",
    "- **Error Handling**: Robust error handling with retry mechanisms\n",
    "- **Data Validation**: Validates scraped data to ensure quality\n",
    "- **CSV Export**: Saves collected data in CSV format\n",
    "- **Progress Tracking**: Real-time progress updates during scraping\n",
    "\n",
    "## Requirements\n",
    "- Python 3.x\n",
    "- Selenium\n",
    "- BeautifulSoup4\n",
    "- Pandas\n",
    "- Chrome WebDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef1a27",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for web scraping, data processing, and file handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ab84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a562517",
   "metadata": {},
   "source": [
    "## 2. Configuration Class\n",
    "\n",
    "The `ScraperConfig` class contains all the configuration parameters for the scraper. You can modify these values according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScraperConfig:\n",
    "    # Path to Chrome WebDriver executable\n",
    "    CHROME_DRIVER_PATH = r\"D:\\DSE_Folder\\Year_3\\Sem_2\\Web Scraping\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "    \n",
    "    # Output CSV filename\n",
    "    OUTPUT_FILENAME = \"New_Data_workinga.csv\"\n",
    "    \n",
    "    # Range of job IDs to scrape\n",
    "    START_ID = 10755\n",
    "    END_ID = 11683\n",
    "    \n",
    "    # Base URL template for job pages\n",
    "    BASE_URL = \"https://workingna.com/job/{}\"\n",
    "    \n",
    "    # Timing and retry configurations\n",
    "    WAIT_TIMEOUT = 1  # seconds to wait for page elements\n",
    "    DELAY = 0.1  # seconds between requests\n",
    "    MAX_RETRIES = 2  # maximum retry attempts\n",
    "    \n",
    "    # CSV column structure\n",
    "    COLUMNS = [\n",
    "        \"Job Title\", \"Company Name\", \"Salary\", \"Available\", \"Office\", \n",
    "        \"Location\", \"Employment Type\", \"Closing Date\", \n",
    "        \"Job Responsibilities\", \"Job Requirements\", \"Link\"\n",
    "    ]\n",
    "\n",
    "print(\"‚úÖ Configuration class defined!\")\n",
    "print(f\"üìä Will scrape job IDs from {ScraperConfig.START_ID} to {ScraperConfig.END_ID}\")\n",
    "print(f\"üíæ Output file: {ScraperConfig.OUTPUT_FILENAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7840b78a",
   "metadata": {},
   "source": [
    "## 3. Main Scraper Class\n",
    "\n",
    "The `JobScraper` class handles all the web scraping functionality. Let's define it step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d86ffd4",
   "metadata": {},
   "source": [
    "### 3.1 Class Initialization and WebDriver Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6781ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobScraper:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.driver = self._init_driver()\n",
    "        self.scraped_count = 0\n",
    "        self.skipped_count = 0\n",
    "        self.error_count = 0\n",
    "        \n",
    "    def _init_driver(self):\n",
    "        \"\"\"Initialize Chrome WebDriver with optimized settings\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  # Run in background\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        service = Service(self.config.CHROME_DRIVER_PATH)\n",
    "        return webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "print(\"‚úÖ JobScraper initialization methods defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22536d84",
   "metadata": {},
   "source": [
    "### 3.2 Text Processing and Validation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead13ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text data\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        # Remove leading/trailing whitespace and normalize internal spaces\n",
    "        text = ' '.join(text.strip().split())\n",
    "        # Remove leading hyphens/dashes if present\n",
    "        if text.startswith('-') or text.startswith('‚Äì'):\n",
    "            text = text[1:].strip()\n",
    "        return text\n",
    "    \n",
    "    def is_page_not_found(self, soup):\n",
    "        \"\"\"Check if the page shows 'not found' or similar error\"\"\"\n",
    "        error_messages = [\n",
    "            \"not found\", \n",
    "            \"404\", \n",
    "            \"page doesn't exist\", \n",
    "            \"job not available\",\n",
    "            \"no longer available\"\n",
    "        ]\n",
    "        page_text = soup.get_text().lower()\n",
    "        return any(msg in page_text for msg in error_messages)\n",
    "    \n",
    "    def is_empty_page(self, job_info):\n",
    "        \"\"\"Check if the page has no meaningful data\"\"\"\n",
    "        required_fields = [\"Job Title\", \"Company Name\", \"Job Responsibilities\"]\n",
    "        return all(job_info.get(field) in [None, \"Not specified\", \"\"] for field in required_fields)\n",
    "\n",
    "# Add methods to the class\n",
    "JobScraper.clean_text = clean_text\n",
    "JobScraper.is_page_not_found = is_page_not_found\n",
    "JobScraper.is_empty_page = is_empty_page\n",
    "\n",
    "print(\"‚úÖ Text processing and validation methods added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b89526",
   "metadata": {},
   "source": [
    "### 3.3 Data Extraction Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def extract_element(self, soup, find_params, next_element=None, attribute=None):\n",
    "        \"\"\"Extract specific elements from HTML soup with error handling\"\"\"\n",
    "        try:\n",
    "            element = soup.find(**find_params)\n",
    "            if not element:\n",
    "                return None\n",
    "                \n",
    "            if next_element:\n",
    "                element = element.find_next(next_element)\n",
    "                if not element:\n",
    "                    return None\n",
    "                    \n",
    "            return element.get(attribute) if attribute else element.text\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def extract_ql_editor_content(self, soup, heading_text):\n",
    "        \"\"\"\n",
    "        Extract content from a div with class=\"ql-editor\" that contains multiple <p> tags\n",
    "        Returns None if content is just placeholder text\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First find the heading\n",
    "            heading = soup.find(lambda tag: tag.name and heading_text.lower() in tag.get_text().lower())\n",
    "            if not heading:\n",
    "                return None\n",
    "            \n",
    "            # Find the ql-editor div after the heading\n",
    "            ql_editor = heading.find_next(\"div\", class_=\"ql-editor\")\n",
    "            if not ql_editor:\n",
    "                return None\n",
    "            \n",
    "            # Extract all <p> tags within the ql-editor\n",
    "            paragraphs = ql_editor.find_all(\"p\")\n",
    "            if not paragraphs:\n",
    "                return None\n",
    "            \n",
    "            # Clean each paragraph and filter out placeholders\n",
    "            items = []\n",
    "            for p in paragraphs:\n",
    "                text = self.clean_text(p.get_text())\n",
    "                if text and text.lower() not in [\"job detail\", \"not specified\"]:\n",
    "                    items.append(text)\n",
    "            \n",
    "            if items:\n",
    "                return \" ‚Ä¢ \".join(items)\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting ql-editor content for '{heading_text}': {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# Add methods to the class\n",
    "JobScraper.extract_element = extract_element\n",
    "JobScraper.extract_ql_editor_content = extract_ql_editor_content\n",
    "\n",
    "print(\"‚úÖ Data extraction methods added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c12f7a6",
   "metadata": {},
   "source": [
    "### 3.4 Section Content Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b7e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def extract_section_content(self, soup, heading_text):\n",
    "        \"\"\"\n",
    "        Extract content from a section with strict validation against placeholder text\n",
    "        \"\"\"\n",
    "        # First try to get ql-editor content\n",
    "        ql_content = self.extract_ql_editor_content(soup, heading_text)\n",
    "        if ql_content:\n",
    "            return ql_content\n",
    "            \n",
    "        # Then try to get multiple paragraphs\n",
    "        try:\n",
    "            heading = soup.find(lambda tag: tag.name and heading_text.lower() in tag.get_text().lower())\n",
    "            if heading:\n",
    "                paragraphs = []\n",
    "                next_tag = heading.find_next_sibling()\n",
    "                \n",
    "                # Collect all consecutive <p> tags until we hit a different element type\n",
    "                while next_tag and next_tag.name == 'p':\n",
    "                    text = self.clean_text(next_tag.get_text())\n",
    "                    if text and text.lower() not in [\"job detail\", \"not specified\"]:\n",
    "                        paragraphs.append(text)\n",
    "                    next_tag = next_tag.find_next_sibling()\n",
    "                \n",
    "                if paragraphs:\n",
    "                    return \" ‚Ä¢ \".join(paragraphs)\n",
    "                \n",
    "                # Try to find a <ul> list\n",
    "                list_section = heading.find_next(\"ul\")\n",
    "                if list_section:\n",
    "                    items = []\n",
    "                    for li in list_section.find_all(\"li\", recursive=False):\n",
    "                        text = self.clean_text(li.text)\n",
    "                        if text and text.lower() not in [\"job detail\", \"not specified\"]:\n",
    "                            items.append(text)\n",
    "                    \n",
    "                    if items:\n",
    "                        return \" ‚Ä¢ \".join(items)\n",
    "                \n",
    "                # Try to find direct <p> tag after heading\n",
    "                p_tag = heading.find_next(\"p\")\n",
    "                if p_tag:\n",
    "                    text = self.clean_text(p_tag.text)\n",
    "                    if text and text.lower() not in [\"job detail\", \"not specified\"]:\n",
    "                        return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting section '{heading_text}': {str(e)}\")\n",
    "        \n",
    "        return \"Not specified\"\n",
    "\n",
    "# Add method to the class\n",
    "JobScraper.extract_section_content = extract_section_content\n",
    "\n",
    "print(\"‚úÖ Section content extraction method added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef79833",
   "metadata": {},
   "source": [
    "### 3.5 Main Scraping Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ad6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def scrape_job_page(self, job_id):\n",
    "        \"\"\"Scrape a single job page and extract all relevant information\"\"\"\n",
    "        url = self.config.BASE_URL.format(job_id)\n",
    "        \n",
    "        for attempt in range(self.config.MAX_RETRIES + 1):\n",
    "            try:\n",
    "                self.driver.get(url)\n",
    "                \n",
    "                # Check for HTTP errors in the URL\n",
    "                if \"404\" in self.driver.title or \"Not Found\" in self.driver.title:\n",
    "                    return None\n",
    "                \n",
    "                # Wait for page to load or detect not found page\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, self.config.WAIT_TIMEOUT).until(\n",
    "                        EC.presence_of_element_located((By.CLASS_NAME, \"MuiBox-root\"))\n",
    "                    )\n",
    "                except TimeoutException:\n",
    "                    # Check if this is a \"not found\" page\n",
    "                    soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "                    if self.is_page_not_found(soup):\n",
    "                        return None\n",
    "                    raise\n",
    "                \n",
    "                time.sleep(self.config.DELAY)\n",
    "                soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "                \n",
    "                # Double check for not found page after load\n",
    "                if self.is_page_not_found(soup):\n",
    "                    return None\n",
    "                \n",
    "                job_info = {col: None for col in self.config.COLUMNS}\n",
    "                \n",
    "                # Extract basic job information\n",
    "                job_info[\"Job Title\"] = self.clean_text(self.extract_element(soup, {\"class_\": \"css-97a38i\"}))\n",
    "                job_info[\"Company Name\"] = self.clean_text(self.extract_element(soup, {\"class_\": \"css-aabkpg\"}, \"h6\"))\n",
    "                job_info[\"Office\"] = self.clean_text(self.extract_element(soup, {\"class_\": \"css-bnbs76\"}, \"p\"))\n",
    "                \n",
    "                # Extract labeled fields\n",
    "                label_fields = {\n",
    "                    \"Location\": \"Location\",\n",
    "                    \"Employment Type\": \"Employment\",\n",
    "                    \"Closing Date\": \"Closing Date\"\n",
    "                }\n",
    "                \n",
    "                for field, label in label_fields.items():\n",
    "                    job_info[field] = self.clean_text(self.extract_element(soup, {\"string\": label}, \"p\"))\n",
    "                \n",
    "                # Extract salary and availability\n",
    "                salary_tag = soup.find(\"span\", class_=\"css-10bh2m3\")\n",
    "                if salary_tag:\n",
    "                    job_info[\"Salary\"] = self.clean_text(salary_tag.text)\n",
    "                    available_text = salary_tag.find_next(\"span\")\n",
    "                    if available_text:\n",
    "                        job_info[\"Available\"] = self.clean_text(available_text.text)\n",
    "                \n",
    "                # Extract job sections with validation\n",
    "                responsibilities = self.extract_section_content(soup, \"JOB RESPONSIBILITIES\")\n",
    "                requirements = self.extract_section_content(soup, \"JOB REQUIREMENTS\")\n",
    "                \n",
    "                # Additional validation to ensure we don't get placeholder text\n",
    "                job_info[\"Job Responsibilities\"] = responsibilities if responsibilities != \"Job Detail\" else \"Not specified\"\n",
    "                job_info[\"Job Requirements\"] = requirements if requirements != \"Job Detail\" else \"Not specified\"\n",
    "                \n",
    "                job_info[\"Link\"] = url\n",
    "                \n",
    "                # Clean None values\n",
    "                job_info = {k: v if v is not None else \"Not specified\" for k, v in job_info.items()}\n",
    "                \n",
    "                # Check if page has meaningful data\n",
    "                if self.is_empty_page(job_info):\n",
    "                    return None\n",
    "                \n",
    "                return job_info\n",
    "                \n",
    "            except TimeoutException:\n",
    "                if attempt == self.config.MAX_RETRIES:\n",
    "                    return None\n",
    "                continue\n",
    "                \n",
    "            except WebDriverException as e:\n",
    "                if attempt == self.config.MAX_RETRIES:\n",
    "                    return None\n",
    "                continue\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt == self.config.MAX_RETRIES:\n",
    "                    return None\n",
    "                continue\n",
    "\n",
    "# Add method to the class\n",
    "JobScraper.scrape_job_page = scrape_job_page\n",
    "\n",
    "print(\"‚úÖ Main scraping method added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c83f8e",
   "metadata": {},
   "source": [
    "### 3.6 Data Saving and Execution Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d21ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def save_to_csv(self, data):\n",
    "        \"\"\"Save scraped data to CSV file\"\"\"\n",
    "        file_exists = os.path.exists(self.config.OUTPUT_FILENAME)\n",
    "        \n",
    "        with open(self.config.OUTPUT_FILENAME, mode=\"a\", newline=\"\", encoding=\"utf-8-sig\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=self.config.COLUMNS)\n",
    "            \n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "                \n",
    "            writer.writerow(data)\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Execute the complete scraping process\"\"\"\n",
    "        print(f\"üöÄ Starting scraping from ID {self.config.START_ID} to {self.config.END_ID}\")\n",
    "        print(f\"üìÅ Output will be saved to {self.config.OUTPUT_FILENAME}\")\n",
    "        print(f\"‚è≥ Timeout set to {self.config.WAIT_TIMEOUT} seconds with {self.config.MAX_RETRIES} retries\\n\")\n",
    "        \n",
    "        for job_id in range(self.config.START_ID, self.config.END_ID + 1):\n",
    "            print(f\"üîç Processing job ID {job_id}...\", end=\" \", flush=True)\n",
    "            \n",
    "            job_data = self.scrape_job_page(job_id)\n",
    "            \n",
    "            if job_data is not None:\n",
    "                self.scraped_count += 1\n",
    "                self.save_to_csv(job_data)\n",
    "                print(f\"‚úÖ Success\")\n",
    "                print(f\"   Title: {job_data['Job Title']}\")\n",
    "                print(f\"   Company: {job_data['Company Name']}\")\n",
    "            else:\n",
    "                self.skipped_count += 1\n",
    "                print(f\"‚è© Skipped (No data or error)\")\n",
    "            \n",
    "            print()  # Add empty line between jobs\n",
    "        \n",
    "        print(\"\\nScraping complete! Summary:\")\n",
    "        print(f\"‚úÖ Successful scrapes: {self.scraped_count}\")\n",
    "        print(f\"‚è© Skipped jobs: {self.skipped_count}\")\n",
    "        print(f\"üíæ Data saved to {self.config.OUTPUT_FILENAME}\")\n",
    "        \n",
    "        self.driver.quit()\n",
    "\n",
    "# Add methods to the class\n",
    "JobScraper.save_to_csv = save_to_csv\n",
    "JobScraper.run = run\n",
    "\n",
    "print(\"‚úÖ Data saving and execution methods added!\")\n",
    "print(\"üéâ JobScraper class is now complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e05b32e",
   "metadata": {},
   "source": [
    "## 4. Execute the Scraper\n",
    "\n",
    "Now let's create an instance of the scraper and run it. **Note**: Make sure your Chrome WebDriver path is correct before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration and scraper instances\n",
    "config = ScraperConfig()\n",
    "scraper = JobScraper(config)\n",
    "\n",
    "print(\"üéØ Scraper initialized successfully!\")\n",
    "print(f\"üìä Ready to scrape {config.END_ID - config.START_ID + 1} job listings\")\n",
    "print(\"\\n‚ö†Ô∏è Make sure the Chrome WebDriver path is correct before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04820e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the scraper\n",
    "# Uncomment the line below to start scraping\n",
    "# scraper.run()\n",
    "\n",
    "print(\"üîÑ To start scraping, uncomment the 'scraper.run()' line above and run this cell.\")\n",
    "print(\"‚ö†Ô∏è Warning: This will take some time depending on the number of job IDs to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b99b6cb",
   "metadata": {},
   "source": [
    "## 5. Data Analysis and Results\n",
    "\n",
    "After scraping is complete, let's analyze the collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa879f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze the scraped data\n",
    "try:\n",
    "    df = pd.read_csv(config.OUTPUT_FILENAME)\n",
    "    \n",
    "    print(f\"üìä Data Analysis for {config.OUTPUT_FILENAME}\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total jobs scraped: {len(df)}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nüìà Basic Statistics:\")\n",
    "    print(f\"Unique companies: {df['Company Name'].nunique()}\")\n",
    "    print(f\"Unique locations: {df['Location'].nunique()}\")\n",
    "    print(f\"Jobs with specified salary: {df[df['Salary'] != 'Not specified'].shape[0]}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå No data file found. Please run the scraper first.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e286e",
   "metadata": {},
   "source": [
    "## 6. Data Export Options\n",
    "\n",
    "Additional options for exporting and viewing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d57a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to different formats if needed\n",
    "try:\n",
    "    df = pd.read_csv(config.OUTPUT_FILENAME)\n",
    "    \n",
    "    # Export to Excel\n",
    "    excel_filename = config.OUTPUT_FILENAME.replace('.csv', '.xlsx')\n",
    "    df.to_excel(excel_filename, index=False)\n",
    "    print(f\"‚úÖ Data exported to Excel: {excel_filename}\")\n",
    "    \n",
    "    # Export to JSON\n",
    "    json_filename = config.OUTPUT_FILENAME.replace('.csv', '.json')\n",
    "    df.to_json(json_filename, orient='records', indent=2)\n",
    "    print(f\"‚úÖ Data exported to JSON: {json_filename}\")\n",
    "    \n",
    "    # Show data types\n",
    "    print(f\"\\nüìã Data Types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå No data file found. Please run the scraper first.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error exporting data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e04795c",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "This Jupyter notebook provides a complete web scraping solution for Workinga.com job listings. The scraper includes:\n",
    "\n",
    "### Key Features:\n",
    "- **Robust Error Handling**: Handles various error scenarios including network issues, missing pages, and invalid data\n",
    "- **Data Validation**: Ensures scraped data quality by filtering out placeholder content\n",
    "- **Configurable Parameters**: Easy to modify scraping parameters through the `ScraperConfig` class\n",
    "- **Progress Tracking**: Real-time updates on scraping progress\n",
    "- **Multiple Export Formats**: CSV, Excel, and JSON output options\n",
    "\n",
    "### Data Extracted:\n",
    "- Job Title\n",
    "- Company Name\n",
    "- Salary Information\n",
    "- Job Location\n",
    "- Employment Type\n",
    "- Job Responsibilities\n",
    "- Job Requirements\n",
    "- Application Deadline\n",
    "- Direct Job Links\n",
    "\n",
    "### Usage Notes:\n",
    "1. Ensure Chrome WebDriver is properly installed and path is correct\n",
    "2. Modify the `START_ID` and `END_ID` in `ScraperConfig` as needed\n",
    "3. Adjust timing parameters if you encounter rate limiting\n",
    "4. The scraper runs in headless mode for better performance\n",
    "\n",
    "### Ethical Considerations:\n",
    "- Always respect the website's robots.txt and terms of service\n",
    "- Use reasonable delays between requests to avoid overwhelming the server\n",
    "- Consider the website's server load and scrape responsibly\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Scraping! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
